## plate pre-processing pipeline walkthrough

# When the data arrives from the HiSeq facility, it may be divided into many folders (one for each sample containing the data and a fastqc report)
# Use "seq_gather.py" to assemble the seq files in one folder and the fastqc files in another. This is a small enough operation that it can be done directly, without sbatch.

python3 seq_gather.py

# Preliminary QC should be done here by checking the size of the .fastas and the size of the unsorted data files (sample files should be 10-100s of MB), and by checking the fastqc htmls for read quality. 

# Next, we remove PCR duplicates with "dedup.py" and "dedup.slurm"
# We will have to change the paths in .slurm (cd into directory with sequence files and call the python code from where it resides)
# The deduplication should be done via sbatch on hummingbird
# utilizes clumpify.sh with dedup flag from bbtools

sbatch dedup.slurm

# Next, we clean/trim with trimmomatic_clean.sh, which removes adapters and reads less than 100bp (everything should be >150 based on size selection on the pooled libraries; plates 1 and 2 need to be re-run as they did not get small read removal.)
# We will have to change the paths (cd into directory with sequence files and call the python code from where it resides)
# The cleaning should be done via sbatch on hummingbird
# This script runs the cleaning in an array for more efficient processing
# utilizes trimmomatic

## trim bad quality bases here too! 

sbatch trimmomatic_clean.sh

# Now we are ready to map, and again this is done through an array script submitted through sbatch.
# The bwa_mapping_paired.sh file needs the directory path updated to the folder with trimmed reads and an indexed reference genome and may also need edits to labels, like "library name"
# For paired output from trimmomatic, we use
# Utilizes bwa mem

sbatch bwa_mapping_paired.sh

# for the unpaired output from trimmomatic we use bwa_mapping_unpaired.sh
# This also needs the path to the folder changed to be for the folder containing post trim fq.qzs and the reference genome and may need other label changes, like "library name"

sbatch bwa_mapping_unpaired.sh

# next we merge the bams with samtools using the bam_merge.slurm script
# remember to change the stated directory to the one containing all the .bam files and change the run name if needed. 
# Note: there is a commented option for running as an array 
# utilizes samtools merge

sbatch bam_merge.slurm

####DON'T DO THIS! It is weirdly inconsistant; need to go back and remove sml reads at fasta trimming stage with p1 and p2##########
OR! there may be small reads that are mapping artifacts (would not disappear by filterint out small reads from fasta) -> if so can filter from the bams to get rid of them. (usually below a size threshold of 35, for ex)
# Next we filter out reads of less than 100bp, as all reads really should be >150 after size selection. 
# we use samtools view and awk to do this. 
# again, the script lilkely needs to be edited based on bam naming conventions and location

sbatch remove_small_reads.sh
###############

# Next we remove duplicates again, this time including optical duplicates (mapping to exact same coordinates).
# Utilizes samtools sort/fixmate/markdups

sbatch markdups_rm.sh

# merge reads for same individual with samtools merge

merge_bams_per_individual.sh

# we can check out coverage with this array script.
# utilizes samtools depth and awk

sbatch coverage_from_bams_loop.sh

########## for generation of gVCFs with GATK ################
# To use GATK, we need to assign readgroups. We will do this with Picard tools
# Assign readgroups using picard tools to bams as required by GATK for vcf generation.

## check what the default parameters of GATK are

asgn_read_groups_p1.sh

#index bams for GATK to run properly

index_bams.sh

# Use GATK4 HaplotypeCaller (which includes realignment of indels) to generate VCF. 

GVCF_GATK_HaplotypeCaller.sh

# Generate map file for running GenomicsDBImport

Map_file_generation.sh

# Merge gvcfs from multiple samples with GenomicsDBImport from GATK 

GenomicsDBImport.sh

# Perform joint genotype calling with GenotypeGVCFs from GATK

GenotypeGVCFs.sh

######### for filtering VCFs to contain shared sites between spp - Nicolas Alexandre code ##################




